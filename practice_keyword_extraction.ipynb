{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from click.testing import CliRunner\n",
    "import sys, os\n",
    "import yake\n",
    "import pandas as pd\n",
    "from tika import parser # pip install tika\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk as nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myPath = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.insert(0, myPath + '/../')\n",
    "# print(myPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDFs and convert to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"practice_pdfs\"\n",
    "files = list(glob.glob(os.path.join(directory,'*.*')))\n",
    "print(files)\n",
    "#https://stackoverflow.com/questions/34000914/how-to-create-a-list-from-filenames-in-a-user-specified-directory-in-python\n",
    "#https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "#https://stackoverflow.com/questions/33912773/python-read-txt-files-into-a-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file\n",
    "\n",
    "document_list = []\n",
    "for f in files:\n",
    "    raw = parser.from_file(f)\n",
    "    document_list.append(raw)\n",
    "\n",
    "# print(document_list)\n",
    "\n",
    "\n",
    "# raw = parser.from_file('CIR.0000000000000749.pdf')\n",
    "# # print(raw['content'])\n",
    "# # print(type(raw))\n",
    "# print(raw.keys())\n",
    "# metadata = raw[\"metadata\"]\n",
    "# content = raw[\"content\"]\n",
    "# # print(metadata)\n",
    "# # print(content)\n",
    "# print(type(content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(document_list)\n",
    "# text_df.head()\n",
    "# print(text_df[\"content\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Keywords with TF-IDF and Python’s Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "text_df['content'] = text_df['content'].apply(lambda x:pre_process(x))\n",
    "\n",
    "#show the second 'text' just for fun\n",
    "text_df['content'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(stop_file_path):\n",
    "#     \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"stop_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the text column \n",
    "docs=text_df['content'].tolist()\n",
    "\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "#imit our vocabulary size to 10,000\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you only needs to do this once, this is a mapping of index to \n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs[0]\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling using Gensim LDA Library on Stemmed Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a technique for taking some unstructured text and automatically extracting its common themes, it is a great way to get a bird's eye view on a large text collection. \n",
    "\n",
    "Gensim = “Generate Similar” is a popular open source natural language processing library used for unsupervised topic modeling. \n",
    "\n",
    "The Gensim library uses a popular algorithm for doing topic model, namely Latent Dirichlet Allocation. Latent Dirichlet Allocation (LDA). LDA requires documents to be represented as a bag of words (for the gensim library, some of the API calls will shorten it to \"bow\"). This representation ignores word ordering in the document but retains information on how many times each word appears.\n",
    "\n",
    "The main distinguishing feature for LDA is it allows for mixed membership, which means that each document can partially belong to several different topics. Note that the vocabulary probability will sum up to 1 for every topic, but often times, words that have lower weights will be truncated from the output.\n",
    "\n",
    "Text modified from: \n",
    "* <https://notebook.community/ethen8181/machine-learning/clustering/topic_model/LDA>\n",
    "* <https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py>\n",
    "* <https://www.tutorialspoint.com/gensim/index.htm>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-process the text by making all terms lower case, remove special characters and numbers\n",
    "##Code from: https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    ##lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    ##remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    ## remove special characters and space, but leave in periods and numbers\n",
    "    #text=re.sub('[^A-Za-z0-9.]+|\\s',' ',text)\n",
    "    \n",
    "    ##remove tags\n",
    "    #text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    ##Remove Emails\n",
    "    #text=re.sub('\\S*@\\S*\\s?', '', text) \n",
    "\n",
    "    ##Remove new line characters\n",
    "    #text=[re.sub('\\s+', ' ', text)\n",
    "\n",
    "    ##Remove distracting single quotes\n",
    "    #text=[re.sub(\"\\'\", \"\", text) \n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "text_df['preprocess'] = text_df['content'].apply(lambda x:pre_process(x))\n",
    "\n",
    "# list(text_df.columns)\n",
    "#show the second 'text' just for fun\n",
    "# text_df['preprocess'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Then break the document text into tokens, remove the stopwords, and stem the tokens\n",
    "##Code from: https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "doc_token_list=[]\n",
    "    \n",
    "def tokenize_stem(documents):\n",
    "    \n",
    "    ##Create PorterStemmer\n",
    "    ##The better stemmer is the SnowballStemmer for English\n",
    "    ##https://www.nltk.org/howto/stem.html\n",
    "    ##p_stemmer = PorterStemmer()\n",
    "    \n",
    "    \n",
    "    ##Create SnowballStemmer\n",
    "    sb_stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "\n",
    "    ##Open stop words text file and save to stop_set variable\n",
    "    with open(\"stop_words.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        f.close()\n",
    "\n",
    "    ##Stopword list comes from the Terrier pacakge with 733 words and another 86 custom terms: \n",
    "    ##https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt\n",
    "    ##https://github.com/kavgan/stop-words/blob/master/minimal-stop.txt\n",
    "    \n",
    "    ##Other stopword list options can be reviewed here:\n",
    "    ##https://medium.com/towards-artificial-intelligence/stop-the-stopwords-using-different-python-libraries-ffa6df941653\n",
    "\n",
    "\n",
    "    for doc in documents.dropna():\n",
    "\n",
    "        # Tokenize documents by splitting into words using NLTK's word_tokenize \n",
    "        token_list = nltk.word_tokenize(doc)\n",
    "\n",
    "        # Remove stop words from token_list\n",
    "        token_nostop_list = [i for i in doc if not i in stop_set]\n",
    "        \n",
    "        # Use Porter Stemmer to stem tokens to create more like-words\n",
    "        #token_stem_list = [p_stemmer.stem(i) for i in token_nostop_list if len(i) > 3]\n",
    "            \n",
    "        # Use Snowball Stemmer to stem tokens to create more like-words\n",
    "        token_stem_list = [sb_stemmer.stem(i) for i in token_nostop_list if len(i) > 3]\n",
    "            \n",
    "        #Append token_stem_list to the doc_token_list\n",
    "        doc_token_list.append(token_stem_list)\n",
    "\n",
    "\n",
    "    return doc_token_list\n",
    "\n",
    "tokenize_stem(text_df['preprocess'])\n",
    "# print(type(doc_token_list))\n",
    "# print(doc_token_list[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the gensim topic modeling and return the topics\n",
    "##Code from: https://notebook.community/ethen8181/machine-learning/clustering/topic_model/LDA\n",
    "\n",
    "def get_gensim_corpus_dictionary(data):\n",
    "    ##If content is not yet a list, make it a list and build the id2word dictionary and the corpus (map the word to id)\n",
    "    ##texts = text_df['content'].apply(lambda x: x.split(' ')).tolist()\n",
    "    ##print(texts)\n",
    "\n",
    "    ##Build the id2word dictionary and the corpus\n",
    "    ##The dictionary associates each word in the corpus with a unique integer ID\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    print('Number of unique tokens: ', len(dictionary))\n",
    "\n",
    "    ## Filter out words that appear in less than 2 documents (appear only once),\n",
    "    dictionary.filter_extremes(no_below = 2)\n",
    "\n",
    "    ## Filter out words that appears in more than certain % of documents\n",
    "    ## no_above = 0.5 would remove words that appear in more than 50% of the documents\n",
    "    # dictionary.filter_extremes(no_above = 0.5)\n",
    "\n",
    "    # Remove gaps in id sequence after words that were removed\n",
    "    dictionary.compactify()\n",
    "    print('Number of unique tokens used 2 or more times: ', len(dictionary))\n",
    "\n",
    "    ##Use code below to print terms in dictionary with their IDs\n",
    "    ##This will show you the number of the terms in the dictionary\n",
    "    #print(\"Dictionary Tokens with ID: \")\n",
    "    #pprint.pprint(dictionary.token2id)\n",
    "    \n",
    "    ##Map terms in corpus to words in dictionary with ID\n",
    "    ##This will show you the ID of the term in the dictionary, and the number of times the terms occurs in the corpus\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in data]\n",
    "    #print(\"Tokens in Corpus with Occurrence: \")\n",
    "    #pprint.pprint(corpus)\n",
    "    \n",
    "    ##Print word count by vector \n",
    "    id_words_count = [[(dictionary[id], count) for id, count in line] for line in bow_corpus]\n",
    "    print(\"Word Count in each Vector: \")\n",
    "    pprint.pprint(id_words_count[2])\n",
    "    \n",
    "     \n",
    "    return bow_corpus, dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bow_corpus, dictionary = get_gensim_corpus_dictionary(doc_token_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Gensim Library LDA Model\n",
    "## See link below if you want to save and load a model\n",
    "## https://notebook.community/ethen8181/machine-learning/clustering/topic_model/LDA\n",
    "\n",
    "def run_gensim_LDA_model(corpus, dictionary):\n",
    "    ##Directory for storing all lda models\n",
    "    model_dir = 'lda_checkpoint'\n",
    "\n",
    "    ##If model_dir directionry is not in the folder, then make the directory\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    ##Load the model if we've already trained it before\n",
    "   \n",
    "    path = os.path.join(model_dir, 'topic_model.lda')\n",
    "    if not os.path.isfile(path):\n",
    "        ##Training LDA can take some time, we could set eval_every = None to not evaluate the model perplexity\n",
    "        ##Other parameters for LdaModel, include: random_state=100, update_every=1,chunksize=100,passes=10,alpha='auto',per_word_topics=True\n",
    "        topic_model = LdaModel(corpus, id2word = dictionary, num_topics = 3, iterations = 200)\n",
    "        topic_model.save(path)\n",
    " \n",
    "    topic_model = LdaModel.load(path)\n",
    "\n",
    "    # Each element of the list is a tuple containing the topic and word / probability list\n",
    "    topics = topic_model.show_topics(num_words = 10, formatted = False)\n",
    "\n",
    "    print(type(topics))\n",
    "    \n",
    "    ##Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. \n",
    "    ##In my experience, topic coherence score, in particular, has been more helpful.\n",
    "    #https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#:~:text=Topic%20Modeling%20is%20a%20technique,in%20the%20Python's%20Gensim%20package.\n",
    "\n",
    "    ## Compute Perplexity\n",
    "    print('\\nPerplexity: ', topic_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    ## Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=topic_model, texts=corpus, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    \n",
    "    \n",
    "    return topics\n",
    "\n",
    "run_gensim_LDA_model(bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topics to CSV\n",
    "\n",
    "def create_topic_CSV(topics):\n",
    "    \n",
    "    ##Create dataframe for topics\n",
    "    df_topics = pd.DataFrame(topics, columns = ['TopicNum', 'Terms'])\n",
    "    #df_topics.head()\n",
    "\n",
    "    ## Save dataframe to csv\n",
    "    with open(r\"topic_modeling.csv\", 'w', encoding='utf-8') as file:\n",
    "        df_topics.to_csv(file)\n",
    "        file.close()\n",
    "    \n",
    "create_topic_CSV(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Gensim Library TFIDF Model \n",
    "##The words that will occur more frequently in the document will get the smaller weights.\n",
    "##https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py\n",
    "##new_list = []\n",
    "\n",
    "tfidf_frequency = []\n",
    "\n",
    "def run_gensim_tfidf_model(corpus, dictionary): \n",
    "    \n",
    "    ##Initialize the tf-idf model, training it on our corpus \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    \n",
    "    ##if working with a new document, you can get tfidf from the model\n",
    "    #new_doc = \"abbott bra adolesc\".lower().split()\n",
    "    #print(new_doc)\n",
    "    #new_list.append(tfidf[dictionary.doc2bow(new_doc)])\n",
    "    \n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    for doc in corpus_tfidf:\n",
    "        ##pprint.pprint(doc)\n",
    "        tfidf_frequency.append(doc)\n",
    "    \n",
    "    #Print word frequencies by vector \n",
    "    id_words_frequency = [[(dictionary[id], frequency) for id, frequency in line] for line in tfidf_frequency]\n",
    "    print(\"Word Frequency by Vector: \")\n",
    "    pprint.pprint(id_words_frequency[2])\n",
    "    \n",
    "run_gensim_tfidf_model(bow_corpus, dictionary)\n",
    "\n",
    "#pprint.pprint(tfidf_frequency)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling using Gensim LDA Library on Lemmatized Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-process the text by making all terms lower case, remove special characters and numbers\n",
    "##Code from: https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "data_pre_process = []\n",
    "\n",
    "def preprocess_data(data): \n",
    "    \n",
    "    for email in data:\n",
    "        \n",
    "        ##lowercase\n",
    "        text_lower=email.lower()\n",
    "        \n",
    "        ##Remove Emails\n",
    "        text_email=re.sub('\\\\S*@\\\\S*\\\\s?', '', text_lower) \n",
    "        \n",
    "        ##remove special characters and digits\n",
    "        text_special=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_email)\n",
    "        \n",
    "  \n",
    "        data_pre_process.append(text_special)\n",
    "    \n",
    "    return data_pre_process\n",
    "\n",
    "preprocess_data(data)\n",
    "print(data_pre_process[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = []\n",
    "\n",
    "def tokenize(documents):\n",
    "    for doc in documents:\n",
    "        token_list = gensim.utils.simple_preprocess(str(doc), deacc=True)  # deacc=True removes punctuations\n",
    "        data_words.append(token_list)\n",
    "    return data_words\n",
    "\n",
    "\n",
    "tokenize(data_pre_process)\n",
    "# print(type(data_words))\n",
    "print(data_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_bigram_trigram_models(documents):\n",
    "    \n",
    "   \n",
    "    ##Building Bigram & Trigram Models\n",
    "    ##higher threshold fewer phrases.\n",
    "    bigram = gensim.models.Phrases(documents, min_count=5, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[documents], threshold=100)\n",
    "        \n",
    "    ##Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "          \n",
    "    ##See trigram example\n",
    "    print(trigram_mod[bigram_mod[doc[0]]])\n",
    "        \n",
    "    return bigram_mod, trigram_mod\n",
    "        \n",
    "bigram_mod, trigram_mod = built_bigram_trigram_models(data_words)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_no_stop_list = []\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "\n",
    "    ##Open stop words text file and save to stop_set variable\n",
    "    with open(\"stop_words.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        f.close()\n",
    "\n",
    "    ##Stopword list comes from the Terrier pacakge with 733 words and another 86 custom terms: \n",
    "    ##https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt\n",
    "    ##https://github.com/kavgan/stop-words/blob/master/minimal-stop.txt\n",
    "    \n",
    "    ##Other stopword list options can be reviewed here:\n",
    "    ##https://medium.com/towards-artificial-intelligence/stop-the-stopwords-using-different-python-libraries-ffa6df941653\n",
    "\n",
    "    for doc in documents:\n",
    "        \n",
    "        # Remove stop words from doc in documents\n",
    "        token_no_stop_list = [i for i in doc if not i in stop_set]\n",
    "        \n",
    "        #Append token_stem_list to the doc_token_list\n",
    "        doc_no_stop_list.append(token_no_stop_list)\n",
    "    \n",
    "    return  doc_no_stop_list \n",
    "            \n",
    "remove_stop_words(doc_token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_list = []\n",
    "\n",
    "def make_bigrams(documents, bigram_mod):\n",
    "    \n",
    "    bigram_list = [bigram_mod[doc] for doc in documents]\n",
    "        \n",
    "    return bigram_list\n",
    "\n",
    "make_bigrams(doc_no_stop_list, bigram_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
