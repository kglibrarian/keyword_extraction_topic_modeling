{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from click.testing import CliRunner\n",
    "import sys, os\n",
    "import yake\n",
    "import pandas as pd\n",
    "from tika import parser # pip install tika\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk as nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myPath = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.insert(0, myPath + '/../')\n",
    "# print(myPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_interface():\n",
    "    text_content = \"\"\"\n",
    "    Sources tell us that Google is acquiring Kaggle, a platform that\n",
    "    hosts data science and machine learning competitions. Details about\n",
    "    the transaction remain somewhat vague , but given that Google is hosting\n",
    "    its Cloud Next conference in San Francisco this week, the official announcement\n",
    "    could come as early    as tomorrow.  Reached by phone, Kaggle co-founder\n",
    "    CEO Anthony Goldbloom declined to deny that the\n",
    "    acquisition is happening. Google itself declined 'to comment on rumors'.\n",
    "    Kaggle, which has about half a million data scientists on its platform,\n",
    "    was founded by Goldbloom    and Ben Hamner in 2010. The service got an\n",
    "    early start and even though it has a few competitors    like DrivenData,\n",
    "    TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its\n",
    "    specific niche. The service is basically the de facto home for running data science\n",
    "    and machine learning    competitions.  With Kaggle, Google is buying one of the largest\n",
    "    and most active communities for    data scientists - and with that, it will get increased\n",
    "    mindshare in this community, too    (though it already has plenty of that thanks to Tensorflow\n",
    "    and other projects).    Kaggle has a bit of a history with Google, too, but that's pretty recent.\n",
    "    Earlier this month,    Google and Kaggle teamed up to host a $100,000 machine learning competition\n",
    "    around classifying    YouTube videos. That competition had some deep integrations with the\n",
    "    Google Cloud Platform, too.    Our understanding is that Google will keep the service running -\n",
    "    likely under its current name.    While the acquisition is probably more about Kaggle's community\n",
    "    than technology, Kaggle did build    some interesting tools for hosting its competition and 'kernels',\n",
    "    too. On Kaggle, kernels are    basically the source code for analyzing data sets and developers can\n",
    "    share this code on the    platform (the company previously called them 'scripts').  Like similar\n",
    "    competition-centric sites,    Kaggle also runs a job board, too. It's unclear what Google will do\n",
    "    with that part of the service.    According to Crunchbase, Kaggle raised $12.5 million (though PitchBook\n",
    "    says it's $12.75) since its    launch in 2010. Investors in Kaggle include Index Ventures, SV Angel,\n",
    "    Max Levchin, Naval Ravikant,    Google chief economist Hal Varian, Khosla Ventures and Yuri Milner\n",
    "    \"\"\"\n",
    "\n",
    "    pyake = yake.KeywordExtractor(lan=\"en\",n=3)\n",
    "\n",
    "    result = pyake.extract_keywords(text_content)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    keywords = [kw[0] for kw in result]\n",
    "\n",
    "    print(keywords)\n",
    "    assert \"google\" in keywords\n",
    "    assert \"kaggle\" in keywords\n",
    "    assert \"san francisco\" in keywords\n",
    "    assert \"machine learning\" in keywords\n",
    "\n",
    "test_simple_interface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_interface():\n",
    "    text_content = \"\"\"\n",
    "    Sources tell us that Google is acquiring Kaggle, a platform that\n",
    "    hosts data science and machine learning competitions.\"\"\"\n",
    "\n",
    "    pyake = yake.KeywordExtractor(lan=\"ca\",n=3)\n",
    "\n",
    "    result = pyake.extract_keywords(text_content)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    assert len(result) > 0\n",
    "\n",
    "test_simple_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_again():\n",
    "    text = \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning \"\\\n",
    "    \"competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud \"\\\n",
    "    \"Next conference in San Francisco this week, the official announcement could come as early as tomorrow. \"\\\n",
    "    \"Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. \"\\\n",
    "    \"Google itself declined 'to comment on rumors'. Kaggle, which has about half a million data scientists on its platform, \"\\\n",
    "    \"was founded by Goldbloom  and Ben Hamner in 2010. \"\\\n",
    "    \"The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, \"\\\n",
    "    \"it has managed to stay well ahead of them by focusing on its specific niche. \"\\\n",
    "    \"The service is basically the de facto home for running data science and machine learning competitions. \"\\\n",
    "    \"With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, \"\\\n",
    "    \"it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow \"\\\n",
    "    \"and other projects). Kaggle has a bit of a history with Google, too, but that's pretty recent. Earlier this month, \"\\\n",
    "    \"Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. \"\\\n",
    "    \"That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google \"\\\n",
    "    \"will keep the service running - likely under its current name. While the acquisition is probably more about \"\\\n",
    "    \"Kaggle's community than technology, Kaggle did build some interesting tools for hosting its competition \"\\\n",
    "    \"and 'kernels', too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can \"\\\n",
    "    \"share this code on the platform (the company previously called them 'scripts'). \"\\\n",
    "    \"Like similar competition-centric sites, Kaggle also runs a job board, too. It's unclear what Google will do with \"\\\n",
    "    \"that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it's $12.75) \"\\\n",
    "    \"since its   launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, \"\\\n",
    "    \"Google chief economist Hal Varian, Khosla Ventures and Yuri Milner \"\n",
    "    \n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "\n",
    "test_again()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_again():\n",
    "    \n",
    "    text = \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning \"\\\n",
    "    \"competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud \"\\\n",
    "    \"Next conference in San Francisco this week, the official announcement could come as early as tomorrow. \"\\\n",
    "    \"Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. \"\\\n",
    "    \"Google itself declined 'to comment on rumors'. Kaggle, which has about half a million data scientists on its platform, \"\\\n",
    "    \"was founded by Goldbloom  and Ben Hamner in 2010. \"\\\n",
    "    \"The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, \"\\\n",
    "    \"it has managed to stay well ahead of them by focusing on its specific niche. \"\\\n",
    "    \"The service is basically the de facto home for running data science and machine learning competitions. \"\\\n",
    "    \"With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, \"\\\n",
    "    \"it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow \"\\\n",
    "    \"and other projects). Kaggle has a bit of a history with Google, too, but that's pretty recent. Earlier this month, \"\\\n",
    "    \"Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. \"\\\n",
    "    \"That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google \"\\\n",
    "    \"will keep the service running - likely under its current name. While the acquisition is probably more about \"\\\n",
    "    \"Kaggle's community than technology, Kaggle did build some interesting tools for hosting its competition \"\\\n",
    "    \"and 'kernels', too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can \"\\\n",
    "    \"share this code on the platform (the company previously called them 'scripts'). \"\\\n",
    "    \"Like similar competition-centric sites, Kaggle also runs a job board, too. It's unclear what Google will do with \"\\\n",
    "    \"that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it's $12.75) \"\\\n",
    "    \"since its   launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, \"\\\n",
    "    \"Google chief economist Hal Varian, Khosla Ventures and Yuri Milner \"\n",
    "    \n",
    "    print(type(text))\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 20\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "\n",
    "test_again()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDFs and convert to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"practice_pdfs\"\n",
    "files = list(glob.glob(os.path.join(directory,'*.*')))\n",
    "print(files)\n",
    "#https://stackoverflow.com/questions/34000914/how-to-create-a-list-from-filenames-in-a-user-specified-directory-in-python\n",
    "#https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "#https://stackoverflow.com/questions/33912773/python-read-txt-files-into-a-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file\n",
    "\n",
    "document_list = []\n",
    "for f in files:\n",
    "    raw = parser.from_file(f)\n",
    "    document_list.append(raw)\n",
    "\n",
    "# print(document_list)\n",
    "\n",
    "\n",
    "# raw = parser.from_file('CIR.0000000000000749.pdf')\n",
    "# # print(raw['content'])\n",
    "# # print(type(raw))\n",
    "# print(raw.keys())\n",
    "# metadata = raw[\"metadata\"]\n",
    "# content = raw[\"content\"]\n",
    "# # print(metadata)\n",
    "# # print(content)\n",
    "# print(type(content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(document_list)\n",
    "# text_df.head()\n",
    "# print(text_df[\"content\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Keywords with TF-IDF and Python’s Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "text_df['content'] = text_df['content'].apply(lambda x:pre_process(x))\n",
    "\n",
    "#show the second 'text' just for fun\n",
    "text_df['content'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(stop_file_path):\n",
    "#     \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"stop_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the text column \n",
    "docs=text_df['content'].tolist()\n",
    "\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "#imit our vocabulary size to 10,000\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you only needs to do this once, this is a mapping of index to \n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs[0]\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Yake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "       \n",
    "    return text\n",
    "\n",
    "\n",
    "text_df['content'] = text_df['content'].apply(lambda x:pre_process(x))\n",
    "\n",
    "#show the second 'text' just for fun\n",
    "text_df['content'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(stop_file_path):\n",
    "#     \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"stop_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and lemmatize abstracts\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text=''\n",
    "for doc in text_df['content'].dropna():\n",
    "    for w in doc.split():\n",
    "        if w not in stopwords:\n",
    "            text=text+' '+ str(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/LIAAD/yake/blob/master/tests/test_yake.py\n",
    "# https://www.sciencedirect.com/science/article/pii/S0020025519308588?via%3Dihub\n",
    "\n",
    "#get the text column \n",
    "# yake_data = text_df['content'].tolist()\n",
    "# print(yake_data)\n",
    "\n",
    "def test_yake(text):\n",
    "    \n",
    "    \n",
    "    print(type(text))\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 20\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "\n",
    "test_yake(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Yake Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_original=''\n",
    "for doc in text_df['content'].dropna():\n",
    "    for w in doc.split():\n",
    "        text_original=text_original+' '+ str(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_yake(text):\n",
    "    \n",
    "    \n",
    "    print(type(text))\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 20\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "\n",
    "test_yake(text_original)\n",
    "\n",
    "#The lower the score, the more relevant the keyword is.\n",
    "#he smaller the value, the more significant the 1-gram term (t) is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling using Gensim LDA Library on Stemmed Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a technique for taking some unstructured text and automatically extracting its common themes, it is a great way to get a bird's eye view on a large text collection. \n",
    "\n",
    "Gensim = “Generate Similar” is a popular open source natural language processing library used for unsupervised topic modeling. \n",
    "\n",
    "The Gensim library uses a popular algorithm for doing topic model, namely Latent Dirichlet Allocation. Latent Dirichlet Allocation (LDA). LDA requires documents to be represented as a bag of words (for the gensim library, some of the API calls will shorten it to \"bow\"). This representation ignores word ordering in the document but retains information on how many times each word appears.\n",
    "\n",
    "The main distinguishing feature for LDA is it allows for mixed membership, which means that each document can partially belong to several different topics. Note that the vocabulary probability will sum up to 1 for every topic, but often times, words that have lower weights will be truncated from the output.\n",
    "\n",
    "Text modified from: \n",
    "* <https://notebook.community/ethen8181/machine-learning/clustering/topic_model/LDA>\n",
    "* <https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py>\n",
    "* <https://www.tutorialspoint.com/gensim/index.htm>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-process the text by making all terms lower case, remove special characters and numbers\n",
    "##Code from: https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    ##lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    ##remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    ## remove special characters and space, but leave in periods and numbers\n",
    "    #text=re.sub('[^A-Za-z0-9.]+|\\s',' ',text)\n",
    "    \n",
    "    ##remove tags\n",
    "    #text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    ##Remove Emails\n",
    "    #text=re.sub('\\S*@\\S*\\s?', '', text) \n",
    "\n",
    "    ##Remove new line characters\n",
    "    #text=[re.sub('\\s+', ' ', text)\n",
    "\n",
    "    ##Remove distracting single quotes\n",
    "    #text=[re.sub(\"\\'\", \"\", text) \n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "text_df['preprocess'] = text_df['content'].apply(lambda x:pre_process(x))\n",
    "\n",
    "# list(text_df.columns)\n",
    "#show the second 'text' just for fun\n",
    "# text_df['preprocess'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Then break the document text into tokens, remove the stopwords, and stem the tokens\n",
    "##Code from: https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "doc_token_list=[]\n",
    "    \n",
    "def tokenize_stem(documents):\n",
    "    \n",
    "    ##Create PorterStemmer\n",
    "    ##The better stemmer is the SnowballStemmer for English\n",
    "    ##https://www.nltk.org/howto/stem.html\n",
    "    ##p_stemmer = PorterStemmer()\n",
    "    \n",
    "    \n",
    "    ##Create SnowballStemmer\n",
    "    sb_stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "\n",
    "    ##Open stop words text file and save to stop_set variable\n",
    "    with open(\"stop_words.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        f.close()\n",
    "\n",
    "    ##Stopword list comes from the Terrier pacakge with 733 words and another 86 custom terms: \n",
    "    ##https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt\n",
    "    ##https://github.com/kavgan/stop-words/blob/master/minimal-stop.txt\n",
    "    \n",
    "    ##Other stopword list options can be reviewed here:\n",
    "    ##https://medium.com/towards-artificial-intelligence/stop-the-stopwords-using-different-python-libraries-ffa6df941653\n",
    "\n",
    "\n",
    "    for doc in documents.dropna():\n",
    "\n",
    "        # Tokenize documents by splitting into words using NLTK's word_tokenize \n",
    "        token_list = nltk.word_tokenize(doc)\n",
    "\n",
    "        # Remove stop words from token_list\n",
    "        token_nostop_list = [i for i in doc if not i in stop_set]\n",
    "        \n",
    "        # Use Porter Stemmer to stem tokens to create more like-words\n",
    "        #token_stem_list = [p_stemmer.stem(i) for i in token_nostop_list if len(i) > 3]\n",
    "            \n",
    "        # Use Snowball Stemmer to stem tokens to create more like-words\n",
    "        token_stem_list = [sb_stemmer.stem(i) for i in token_nostop_list if len(i) > 3]\n",
    "            \n",
    "        #Append token_stem_list to the doc_token_list\n",
    "        doc_token_list.append(token_stem_list)\n",
    "\n",
    "\n",
    "    return doc_token_list\n",
    "\n",
    "tokenize_stem(text_df['preprocess'])\n",
    "# print(type(doc_token_list))\n",
    "# print(doc_token_list[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the gensim topic modeling and return the topics\n",
    "##Code from: https://notebook.community/ethen8181/machine-learning/clustering/topic_model/LDA\n",
    "\n",
    "def get_gensim_corpus_dictionary(data):\n",
    "    ##If content is not yet a list, make it a list and build the id2word dictionary and the corpus (map the word to id)\n",
    "    ##texts = text_df['content'].apply(lambda x: x.split(' ')).tolist()\n",
    "    ##print(texts)\n",
    "\n",
    "    ##Build the id2word dictionary and the corpus\n",
    "    ##The dictionary associates each word in the corpus with a unique integer ID\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    print('Number of unique tokens: ', len(dictionary))\n",
    "\n",
    "    ## Filter out words that appear in less than 2 documents (appear only once),\n",
    "    dictionary.filter_extremes(no_below = 2)\n",
    "\n",
    "    ## Filter out words that appears in more than certain % of documents\n",
    "    ## no_above = 0.5 would remove words that appear in more than 50% of the documents\n",
    "    # dictionary.filter_extremes(no_above = 0.5)\n",
    "\n",
    "    # Remove gaps in id sequence after words that were removed\n",
    "    dictionary.compactify()\n",
    "    print('Number of unique tokens used 2 or more times: ', len(dictionary))\n",
    "\n",
    "    ##Use code below to print terms in dictionary with their IDs\n",
    "    ##This will show you the number of the terms in the dictionary\n",
    "    #print(\"Dictionary Tokens with ID: \")\n",
    "    #pprint.pprint(dictionary.token2id)\n",
    "    \n",
    "    ##Map terms in corpus to words in dictionary with ID\n",
    "    ##This will show you the ID of the term in the dictionary, and the number of times the terms occurs in the corpus\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in data]\n",
    "    #print(\"Tokens in Corpus with Occurrence: \")\n",
    "    #pprint.pprint(corpus)\n",
    "    \n",
    "    ##Print word count by vector \n",
    "    id_words_count = [[(dictionary[id], count) for id, count in line] for line in bow_corpus]\n",
    "    print(\"Word Count in each Vector: \")\n",
    "    pprint.pprint(id_words_count[2])\n",
    "    \n",
    "     \n",
    "    return bow_corpus, dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bow_corpus, dictionary = get_gensim_corpus_dictionary(doc_token_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Gensim Library LDA Model\n",
    "## See link below if you want to save and load a model\n",
    "## https://notebook.community/ethen8181/machine-learning/clustering/topic_model/LDA\n",
    "\n",
    "def run_gensim_LDA_model(corpus, dictionary):\n",
    "    ##Directory for storing all lda models\n",
    "    model_dir = 'lda_checkpoint'\n",
    "\n",
    "    ##If model_dir directionry is not in the folder, then make the directory\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    ##Load the model if we've already trained it before\n",
    "   \n",
    "    path = os.path.join(model_dir, 'topic_model.lda')\n",
    "    if not os.path.isfile(path):\n",
    "        ##Training LDA can take some time, we could set eval_every = None to not evaluate the model perplexity\n",
    "        ##Other parameters for LdaModel, include: random_state=100, update_every=1,chunksize=100,passes=10,alpha='auto',per_word_topics=True\n",
    "        topic_model = LdaModel(corpus, id2word = dictionary, num_topics = 3, iterations = 200)\n",
    "        topic_model.save(path)\n",
    " \n",
    "    topic_model = LdaModel.load(path)\n",
    "\n",
    "    # Each element of the list is a tuple containing the topic and word / probability list\n",
    "    topics = topic_model.show_topics(num_words = 10, formatted = False)\n",
    "\n",
    "    print(type(topics))\n",
    "    \n",
    "    ##Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. \n",
    "    ##In my experience, topic coherence score, in particular, has been more helpful.\n",
    "    #https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#:~:text=Topic%20Modeling%20is%20a%20technique,in%20the%20Python's%20Gensim%20package.\n",
    "\n",
    "    ## Compute Perplexity\n",
    "    print('\\nPerplexity: ', topic_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    ## Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=topic_model, texts=corpus, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    \n",
    "    \n",
    "    return topics\n",
    "\n",
    "run_gensim_LDA_model(bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topics to CSV\n",
    "\n",
    "def create_topic_CSV(topics):\n",
    "    \n",
    "    ##Create dataframe for topics\n",
    "    df_topics = pd.DataFrame(topics, columns = ['TopicNum', 'Terms'])\n",
    "    #df_topics.head()\n",
    "\n",
    "    ## Save dataframe to csv\n",
    "    with open(r\"topic_modeling.csv\", 'w', encoding='utf-8') as file:\n",
    "        df_topics.to_csv(file)\n",
    "        file.close()\n",
    "    \n",
    "create_topic_CSV(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the Gensim Library TFIDF Model \n",
    "##The words that will occur more frequently in the document will get the smaller weights.\n",
    "##https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py\n",
    "##new_list = []\n",
    "\n",
    "tfidf_frequency = []\n",
    "\n",
    "def run_gensim_tfidf_model(corpus, dictionary): \n",
    "    \n",
    "    ##Initialize the tf-idf model, training it on our corpus \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    \n",
    "    ##if working with a new document, you can get tfidf from the model\n",
    "    #new_doc = \"abbott bra adolesc\".lower().split()\n",
    "    #print(new_doc)\n",
    "    #new_list.append(tfidf[dictionary.doc2bow(new_doc)])\n",
    "    \n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    for doc in corpus_tfidf:\n",
    "        ##pprint.pprint(doc)\n",
    "        tfidf_frequency.append(doc)\n",
    "    \n",
    "    #Print word frequencies by vector \n",
    "    id_words_frequency = [[(dictionary[id], frequency) for id, frequency in line] for line in tfidf_frequency]\n",
    "    print(\"Word Frequency by Vector: \")\n",
    "    pprint.pprint(id_words_frequency[2])\n",
    "    \n",
    "run_gensim_tfidf_model(bow_corpus, dictionary)\n",
    "\n",
    "#pprint.pprint(tfidf_frequency)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling using Gensim LDA Library on Lemmatized Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-process the text by making all terms lower case, remove special characters and numbers\n",
    "##Code from: https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "data_pre_process = []\n",
    "\n",
    "def preprocess_data(data): \n",
    "    \n",
    "    for email in data:\n",
    "        \n",
    "        ##lowercase\n",
    "        text_lower=email.lower()\n",
    "        \n",
    "        ##Remove Emails\n",
    "        text_email=re.sub('\\\\S*@\\\\S*\\\\s?', '', text_lower) \n",
    "        \n",
    "        ##remove special characters and digits\n",
    "        text_special=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_email)\n",
    "        \n",
    "  \n",
    "        data_pre_process.append(text_special)\n",
    "    \n",
    "    return data_pre_process\n",
    "\n",
    "preprocess_data(data)\n",
    "print(data_pre_process[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = []\n",
    "\n",
    "def tokenize(documents):\n",
    "    for doc in documents:\n",
    "        token_list = gensim.utils.simple_preprocess(str(doc), deacc=True)  # deacc=True removes punctuations\n",
    "        data_words.append(token_list)\n",
    "    return data_words\n",
    "\n",
    "\n",
    "tokenize(data_pre_process)\n",
    "# print(type(data_words))\n",
    "print(data_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def built_bigram_trigram_models(documents):\n",
    "    \n",
    "   \n",
    "    ##Building Bigram & Trigram Models\n",
    "    ##higher threshold fewer phrases.\n",
    "    bigram = gensim.models.Phrases(documents, min_count=5, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[documents], threshold=100)\n",
    "        \n",
    "    ##Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "          \n",
    "    ##See trigram example\n",
    "    print(trigram_mod[bigram_mod[doc[0]]])\n",
    "        \n",
    "    return bigram_mod, trigram_mod\n",
    "        \n",
    "bigram_mod, trigram_mod = built_bigram_trigram_models(data_words)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_no_stop_list = []\n",
    "\n",
    "def remove_stop_words(documents):\n",
    "\n",
    "    ##Open stop words text file and save to stop_set variable\n",
    "    with open(\"stop_words.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        f.close()\n",
    "\n",
    "    ##Stopword list comes from the Terrier pacakge with 733 words and another 86 custom terms: \n",
    "    ##https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt\n",
    "    ##https://github.com/kavgan/stop-words/blob/master/minimal-stop.txt\n",
    "    \n",
    "    ##Other stopword list options can be reviewed here:\n",
    "    ##https://medium.com/towards-artificial-intelligence/stop-the-stopwords-using-different-python-libraries-ffa6df941653\n",
    "\n",
    "    for doc in documents:\n",
    "        \n",
    "        # Remove stop words from doc in documents\n",
    "        token_no_stop_list = [i for i in doc if not i in stop_set]\n",
    "        \n",
    "        #Append token_stem_list to the doc_token_list\n",
    "        doc_no_stop_list.append(token_no_stop_list)\n",
    "    \n",
    "    return  doc_no_stop_list \n",
    "            \n",
    "remove_stop_words(doc_token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_list = []\n",
    "\n",
    "def make_bigrams(documents, bigram_mod):\n",
    "    \n",
    "    bigram_list = [bigram_mod[doc] for doc in documents]\n",
    "        \n",
    "    return bigram_list\n",
    "\n",
    "make_bigrams(doc_no_stop_list, bigram_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
