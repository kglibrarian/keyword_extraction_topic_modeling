{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice with YAKE! Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAKE! is a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Yake! does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. \n",
    "\n",
    "NOTE: My understanding is that YAKE! is meant to be run on a **single document as a string**. If you have multiple documents, you need to merge them into one single string before using YAKE!\n",
    "\n",
    "#### **YAKE! features to note:**\n",
    "\n",
    "* **Corpus-Independent:** YAKE! offers a solution which can retrieve keywords from a single document only, without the need to rely on external document collection statistics as IDF does; i.e., it can be applied to any text.\n",
    "\n",
    "* **Domain and Language-Independent:** YAKE! works with domains and languages for which there are no ready keyword extraction systems, as it neither requires a training corpus nor depends on sophisticated external sources (such as WordNet or Wikipedia) or linguistic tools (such as NER or PoS taggers) other than a static list of stopwords.\n",
    "\n",
    "* **Interior Stopwords:** YAKE! can retrieve keywords containing interior stopwords (e.g., “game of Thrones”) with higher precision than the state-of-the-art methods.\n",
    "\n",
    "* **Scale:** YAKE! scales to any document length linearly in the number of candidate terms identified.\n",
    "\n",
    "* **Term Frequency-free:** meaning that no conditions are set with respect to the minimum frequency or sentence frequency that a candidate keyword must have. Therefore, based on the features used, a keyword may be considered significant or insignificant with either one occurrence or with multiple occurrences.\n",
    "\n",
    "#### **YAKE! has five main steps:** \n",
    "\n",
    "1. **Text pre-processing and candidate term identification.** The first step pre-processes the document into a machine-readable format in order to identify potential candidate terms.This is an important and crucial step to identify better candidate terms and thus to improve the effectiveness of the algorithm.\n",
    "\n",
    "2. **Feature extraction.** The second phase takes as input a list of individual terms and represents them by a set of statistical features.\n",
    "\n",
    "3. **Computing term score.** In the third step, these features are heuristically combined into a single score likely to reflect the importance of the term.\n",
    "\n",
    "4. **n-gram generation and computing candidate keyword score.** The fourth step then generates the candidate keywords (through an n-gram7 construction methodology) and assigns them scores, based on their importance.\n",
    "\n",
    "5. **Data deduplication and ranking.** Finally, the fifth step compares likely similar keywords through the application of a deduplication distance similarity measure. The list of final keywords is then sorted by their relevance scores. \n",
    "\n",
    "     \n",
    "\n",
    "#### **YAKE! References:**\n",
    "\n",
    "* Yake! on Github: <https://github.com/LIAAD/yake/blob/master/tests/test_yake.py>\n",
    "* Yake! publication: <https://www.sciencedirect.com/science/article/pii/S0020025519308588>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "import sys, os\n",
    "import yake\n",
    "import pandas as pd\n",
    "from tika import parser # pip install tika\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load my dataset\n",
    "Load the list of pdfs, convert the pdfs to text files, and create a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"practice_pdfs\"\n",
    "files = list(glob.glob(os.path.join(directory,'*.*')))\n",
    "print(files)\n",
    "#https://stackoverflow.com/questions/34000914/how-to-create-a-list-from-filenames-in-a-user-specified-directory-in-python\n",
    "#https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "#https://stackoverflow.com/questions/33912773/python-read-txt-files-into-a-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files, convert from PDF to text file, append each file to a document list\n",
    "#https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file\n",
    "\n",
    "document_list = []\n",
    "for f in files:\n",
    "    raw = parser.from_file(f)\n",
    "    document_list.append(raw)\n",
    "\n",
    "# print(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe form the document list\n",
    "text_df = pd.DataFrame(document_list)\n",
    "text_df.head()\n",
    "# print(text_df[\"content\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test YAKE! on my dataset\n",
    "This option uses the internal YAKE! preprocessing and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Yake on one document in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test YAKE on one document in dataframe\n",
    "\n",
    "def test_one_yake(text):\n",
    "    \n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "\n",
    "test_one_yake(text_df['content'][1])\n",
    "\n",
    "#The lower the score, the more relevant the keyword is.\n",
    "#he smaller the value, the more significant the 1-gram term (t) is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Yake on all documents in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test YAKE on all documents in dataframe\n",
    "#Change dataframe content column in to one long string\n",
    "\n",
    "def prepare_text(text):\n",
    "    \n",
    "    text.dropna(inplace = True)\n",
    "    \n",
    "    #initialize empty string\n",
    "    global string_for_yake\n",
    "    \n",
    "    # create string using list comprehension \n",
    "    string_for_yake = ' '.join(text_df['content'].tolist()) \n",
    "    \n",
    "    return string_for_yake\n",
    "\n",
    "prepare_text(text_df) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Yake on all-document string\n",
    "\n",
    "def test_yake(text):\n",
    "    \n",
    "    global keywords\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 200\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                                n=max_ngram_size, \n",
    "                                                dedupLim=deduplication_thresold, \n",
    "                                                dedupFunc=deduplication_algo, \n",
    "                                                windowsSize=windowSize, \n",
    "                                                top=numOfKeywords, \n",
    "                                                features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "\n",
    "test_yake(string_for_yake)\n",
    "\n",
    "#The lower the score, the more relevant the keyword is.\n",
    "#he smaller the value, the more significant the 1-gram term (t) is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save keywords to CSV\n",
    "\n",
    "def create_keyword_CSV(keywords):\n",
    "     \n",
    "    ## Create new dataframe with keywords\n",
    "    keywords_df = pd.DataFrame(keywords)\n",
    "\n",
    "    ## Save dataframe to csv\n",
    "    with open(r\"yake_all_documents_only.csv\", 'w', encoding='utf-8') as file:\n",
    "        keywords_df.to_csv(file)\n",
    "        file.close()\n",
    "    \n",
    "create_keyword_CSV(keywords)\n",
    "\n",
    "## In Excel, use the TRIM() function to change the relevance scores to numbers \n",
    "## and then sort by \"Sort numbers and numbers stored as text separately\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test YAKE! on my dataset\n",
    "\n",
    "This option uses EXTERNAL pre-processing and stopwords and lemmatization PRIOR to using YAKE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-process the text by lowcase, remove emails, remove URLS, remove special characters and numbers\n",
    "## https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    # Lowercase\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Remove Emails\n",
    "    text_email = re.sub('\\\\S*@\\\\S*\\\\s?', '', text_lower) \n",
    "    \n",
    "    # Remove URLS\n",
    "    text_urls = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text_email, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove all white \\t spaces, new lines \\n and tabs \\t\n",
    "    text_spaces = re.sub('\\s+',' ',text_urls)\n",
    "    \n",
    "    # Remove \\n from text\n",
    "    text_space_character = text_spaces.replace('\\n','')\n",
    "    \n",
    "    # Remove \\t from text\n",
    "    text_tab_character = text_space_character.replace('\\t','')\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text_numbers = re.sub(\"(\\\\d|\\\\W)+\",\" \",text_tab_character)\n",
    "    \n",
    "    # Remove tags\n",
    "    text_final = re.sub(\"\",\"\",text_numbers)\n",
    "\n",
    "    # Remove special characters and space, but leave in periods and numbers\n",
    "    #text_special = re.sub('[^A-Za-z0-9.]+|\\s',' ',text_tab_character)\n",
    "    \n",
    "    return text_final\n",
    "\n",
    "## New column \"preprocess\" is formed from applying pre_process function to each item in the \"content\" column in dataframe\n",
    "text_df['preprocess'] = text_df['content'].apply(lambda x:pre_process(x))\n",
    "\n",
    "# print(text_df['preprocess'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get stopwords\n",
    "def get_stop_words(stop_file_path):\n",
    "#     \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"stop_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize and lemmatize documents\n",
    "\n",
    "def split_stop_lemmatize(stopwords, doc_list):\n",
    "    \n",
    "    #initiate a lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    #initiate an empty string\n",
    "    lemmatized_text=''\n",
    "\n",
    "    #split each doc into words\n",
    "    for word in doc_list.split():\n",
    "            \n",
    "        #check if each word is in stopword list and lemmatize, add to string\n",
    "        if word not in stopwords:\n",
    "            lemmatized_text = lemmatized_text+' '+ str(lemmatizer.lemmatize(word))\n",
    "                \n",
    "    return lemmatized_text\n",
    "            \n",
    "## New column \"lemmatized\" is formed from applying pre_process function to each item in the \"preprocess\" column in dataframe\n",
    "\n",
    "text_df['lemmatized'] = text_df['preprocess'].apply(lambda x:split_stop_lemmatize(stopwords, x))\n",
    "\n",
    "print(text_df['lemmatized'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the \"lemmatized\" column in dataframe to one long string\n",
    "\n",
    "def convert_lemmatized_to_string(text_df):\n",
    "    \n",
    "    global lemmatized_string_for_yake\n",
    "    lemmatized_string_for_yake = ' '.join(text_df['lemmatized'].tolist())\n",
    "\n",
    "    return lemmatized_string_for_yake\n",
    "\n",
    "convert_lemmatized_to_string(text_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Yake on pre-processed, lemmatized string\n",
    "## https://github.com/LIAAD/yake/blob/master/tests/test_yake.py\n",
    "## https://www.sciencedirect.com/science/article/pii/S0020025519308588?via%3Dihub\n",
    "\n",
    "def test_yake_preprocessed_lemmatized(text):\n",
    "    global keywords\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 200\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                                n=max_ngram_size, \n",
    "                                                dedupLim=deduplication_thresold, \n",
    "                                                dedupFunc=deduplication_algo, \n",
    "                                                windowsSize=windowSize, \n",
    "                                                top=numOfKeywords, \n",
    "                                                features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "#     for kw in keywords:\n",
    "#         print(kw)\n",
    "    return keywords\n",
    "\n",
    "test_yake_preprocessed_lemmatized(lemmatized_string_for_yake)\n",
    "\n",
    "## The lower the score, the more relevant the keyword is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save keywords to CSV\n",
    "\n",
    "def create_keyword_CSV(keywords):\n",
    "     \n",
    "    ## Create new dataframe with keywords\n",
    "    keywords_df = pd.DataFrame(keywords)\n",
    "\n",
    "    ## Save dataframe to csv\n",
    "    with open(r\"yake_all_documents_preprocess_lemmatization.csv\", 'w', encoding='utf-8') as file:\n",
    "        keywords_df.to_csv(file)\n",
    "        file.close()\n",
    "    \n",
    "create_keyword_CSV(keywords)\n",
    "\n",
    "## In Excel, use the TRIM() function to change the relevance scores to numbers \n",
    "## and then sort by \"Sort numbers and numbers stored as text separately\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test YAKE! on my dataset\n",
    "\n",
    "This option runs Yake individual on each document in dataframe and uses external pre-processing ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-process the text by lowcase, remove emails, remove URLS, remove special characters and numbers\n",
    "#https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "def pre_process(text):\n",
    "    \n",
    "    # Lowercase\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Remove Emails\n",
    "    text_email = re.sub('\\\\S*@\\\\S*\\\\s?', '', text_lower) \n",
    "    \n",
    "    # Remove URLS\n",
    "    text_urls = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text_email, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove all white \\t spaces, new lines \\n and tabs \\t\n",
    "    text_spaces = re.sub('\\s+',' ',text_urls)\n",
    "    \n",
    "    # Remove \\n from text\n",
    "    text_space_character = text_spaces.replace('\\n','')\n",
    "    \n",
    "    # Remove \\t from text\n",
    "    text_tab_character = text_space_character.replace('\\t','')\n",
    "    \n",
    "    # Remove special characters and space, but leave in periods and numbers\n",
    "    text_special = re.sub('[^A-Za-z0-9.]+|\\s',' ',text_tab_character)\n",
    "    \n",
    "    # Remove tags\n",
    "    text_final = re.sub(\"\",\"\",text_special)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    #text_numbers = re.sub(\"(\\\\d|\\\\W)+\",\" \",text_spaces)\n",
    "    \n",
    "    \n",
    "    return text_final\n",
    "\n",
    "## New column \"preprocess\" is formed from applying pre_process function to each item in the \"content\" column in dataframe\n",
    "text_df['preprocess_only'] = text_df['content'].apply(lambda x:pre_process(x))\n",
    "\n",
    "print(text_df['preprocess_only'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test YAKE on all documents in dataframe\n",
    "#Change dataframe content column in to one long string\n",
    "\n",
    "def prepare_text(text):\n",
    "    \n",
    "    text.dropna(inplace = True)\n",
    "    \n",
    "    #initialize empty string\n",
    "    global string_for_yake\n",
    "    \n",
    "    # create string using list comprehension \n",
    "    string_for_yake = ' '.join(text_df['preprocess_only'].tolist()) \n",
    "    \n",
    "    return string_for_yake\n",
    "\n",
    "prepare_text(text_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_yake_preprocessed_only(text):\n",
    "    \n",
    "    #print(type(text))\n",
    "    global keywords\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 200\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, \n",
    "                                                n=max_ngram_size, \n",
    "                                                dedupLim=deduplication_thresold, \n",
    "                                                dedupFunc=deduplication_algo, \n",
    "                                                windowsSize=windowSize, \n",
    "                                                top=numOfKeywords, \n",
    "                                                features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "#     for kw in keywords:\n",
    "#         print(kw)\n",
    "    return keywords\n",
    "\n",
    "test_yake_preprocessed_only(string_for_yake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save keywords to CSV\n",
    "\n",
    "def create_keyword_CSV(keywords):\n",
    "     \n",
    "    ## Create new dataframe with keywords\n",
    "    keywords_df = pd.DataFrame(keywords)\n",
    "\n",
    "    ## Save dataframe to csv\n",
    "    with open(r\"yake_all_documents_preprocess_only.csv\", 'w', encoding='utf-8') as file:\n",
    "        keywords_df.to_csv(file)\n",
    "        file.close()\n",
    "    \n",
    "create_keyword_CSV(keywords)\n",
    "\n",
    "## In Excel, use the TRIM() function to change the relevance scores to numbers \n",
    "## and then sort by \"Sort numbers and numbers stored as text separately\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test YAKE! \n",
    "This option uses text from YAKE tutorials in github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_interface_1():\n",
    "    text_content = \"\"\"\n",
    "    Sources tell us that Google is acquiring Kaggle, a platform that\n",
    "    hosts data science and machine learning competitions. Details about\n",
    "    the transaction remain somewhat vague , but given that Google is hosting\n",
    "    its Cloud Next conference in San Francisco this week, the official announcement\n",
    "    could come as early    as tomorrow.  Reached by phone, Kaggle co-founder\n",
    "    CEO Anthony Goldbloom declined to deny that the\n",
    "    acquisition is happening. Google itself declined 'to comment on rumors'.\n",
    "    Kaggle, which has about half a million data scientists on its platform,\n",
    "    was founded by Goldbloom    and Ben Hamner in 2010. The service got an\n",
    "    early start and even though it has a few competitors    like DrivenData,\n",
    "    TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its\n",
    "    specific niche. The service is basically the de facto home for running data science\n",
    "    and machine learning    competitions.  With Kaggle, Google is buying one of the largest\n",
    "    and most active communities for    data scientists - and with that, it will get increased\n",
    "    mindshare in this community, too    (though it already has plenty of that thanks to Tensorflow\n",
    "    and other projects).    Kaggle has a bit of a history with Google, too, but that's pretty recent.\n",
    "    Earlier this month,    Google and Kaggle teamed up to host a $100,000 machine learning competition\n",
    "    around classifying    YouTube videos. That competition had some deep integrations with the\n",
    "    Google Cloud Platform, too.    Our understanding is that Google will keep the service running -\n",
    "    likely under its current name.    While the acquisition is probably more about Kaggle's community\n",
    "    than technology, Kaggle did build    some interesting tools for hosting its competition and 'kernels',\n",
    "    too. On Kaggle, kernels are    basically the source code for analyzing data sets and developers can\n",
    "    share this code on the    platform (the company previously called them 'scripts').  Like similar\n",
    "    competition-centric sites,    Kaggle also runs a job board, too. It's unclear what Google will do\n",
    "    with that part of the service.    According to Crunchbase, Kaggle raised $12.5 million (though PitchBook\n",
    "    says it's $12.75) since its    launch in 2010. Investors in Kaggle include Index Ventures, SV Angel,\n",
    "    Max Levchin, Naval Ravikant,    Google chief economist Hal Varian, Khosla Ventures and Yuri Milner\n",
    "    \"\"\"\n",
    "\n",
    "    pyake = yake.KeywordExtractor(lan=\"en\",n=3)\n",
    "\n",
    "    result = pyake.extract_keywords(text_content)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    keywords = [kw[0] for kw in result]\n",
    "\n",
    "    print(keywords)\n",
    "    assert \"google\" in keywords\n",
    "    assert \"kaggle\" in keywords\n",
    "    assert \"san francisco\" in keywords\n",
    "    assert \"machine learning\" in keywords\n",
    "\n",
    "test_simple_interface_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_interface_2():\n",
    "    text_content = \"\"\"\n",
    "    Sources tell us that Google is acquiring Kaggle, a platform that\n",
    "    hosts data science and machine learning competitions.\"\"\"\n",
    "\n",
    "    pyake = yake.KeywordExtractor(lan=\"ca\",n=3)\n",
    "\n",
    "    result = pyake.extract_keywords(text_content)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    assert len(result) > 0\n",
    "\n",
    "test_simple_interface_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_interface_3():\n",
    "    text = \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning \"\\\n",
    "    \"competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud \"\\\n",
    "    \"Next conference in San Francisco this week, the official announcement could come as early as tomorrow. \"\\\n",
    "    \"Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. \"\\\n",
    "    \"Google itself declined 'to comment on rumors'. Kaggle, which has about half a million data scientists on its platform, \"\\\n",
    "    \"was founded by Goldbloom  and Ben Hamner in 2010. \"\\\n",
    "    \"The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, \"\\\n",
    "    \"it has managed to stay well ahead of them by focusing on its specific niche. \"\\\n",
    "    \"The service is basically the de facto home for running data science and machine learning competitions. \"\\\n",
    "    \"With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, \"\\\n",
    "    \"it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow \"\\\n",
    "    \"and other projects). Kaggle has a bit of a history with Google, too, but that's pretty recent. Earlier this month, \"\\\n",
    "    \"Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. \"\\\n",
    "    \"That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google \"\\\n",
    "    \"will keep the service running - likely under its current name. While the acquisition is probably more about \"\\\n",
    "    \"Kaggle's community than technology, Kaggle did build some interesting tools for hosting its competition \"\\\n",
    "    \"and 'kernels', too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can \"\\\n",
    "    \"share this code on the platform (the company previously called them 'scripts'). \"\\\n",
    "    \"Like similar competition-centric sites, Kaggle also runs a job board, too. It's unclear what Google will do with \"\\\n",
    "    \"that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it's $12.75) \"\\\n",
    "    \"since its   launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, \"\\\n",
    "    \"Google chief economist Hal Varian, Khosla Ventures and Yuri Milner \"\n",
    "    \n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "\n",
    "test_simple_interface_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_interface_4():\n",
    "    \n",
    "    text = \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning \"\\\n",
    "    \"competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud \"\\\n",
    "    \"Next conference in San Francisco this week, the official announcement could come as early as tomorrow. \"\\\n",
    "    \"Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. \"\\\n",
    "    \"Google itself declined 'to comment on rumors'. Kaggle, which has about half a million data scientists on its platform, \"\\\n",
    "    \"was founded by Goldbloom  and Ben Hamner in 2010. \"\\\n",
    "    \"The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, \"\\\n",
    "    \"it has managed to stay well ahead of them by focusing on its specific niche. \"\\\n",
    "    \"The service is basically the de facto home for running data science and machine learning competitions. \"\\\n",
    "    \"With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, \"\\\n",
    "    \"it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow \"\\\n",
    "    \"and other projects). Kaggle has a bit of a history with Google, too, but that's pretty recent. Earlier this month, \"\\\n",
    "    \"Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. \"\\\n",
    "    \"That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google \"\\\n",
    "    \"will keep the service running - likely under its current name. While the acquisition is probably more about \"\\\n",
    "    \"Kaggle's community than technology, Kaggle did build some interesting tools for hosting its competition \"\\\n",
    "    \"and 'kernels', too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can \"\\\n",
    "    \"share this code on the platform (the company previously called them 'scripts'). \"\\\n",
    "    \"Like similar competition-centric sites, Kaggle also runs a job board, too. It's unclear what Google will do with \"\\\n",
    "    \"that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it's $12.75) \"\\\n",
    "    \"since its   launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, \"\\\n",
    "    \"Google chief economist Hal Varian, Khosla Ventures and Yuri Milner \"\n",
    "    \n",
    "    print(type(text))\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_thresold = 0.9\n",
    "    deduplication_algo = 'seqm'\n",
    "    windowSize = 1\n",
    "    numOfKeywords = 20\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw in keywords:\n",
    "        print(kw)\n",
    "\n",
    "test_simple_interface_4()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
