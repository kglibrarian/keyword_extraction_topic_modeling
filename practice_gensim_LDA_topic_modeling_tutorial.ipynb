{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice with the Gensium Tutorial for LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim Tutorial: <https://www.tutorialspoint.com/gensim/index.htm>\n",
    "\n",
    "Gensim = “Generate Similar” is a popular open source natural language processing library used for unsupervised topic modeling.\n",
    "\n",
    "Gensim uses top academic models and modern statistical machine learning to perform various complex tasks such as −\n",
    "\n",
    "* Building document or word vectors\n",
    "* Corpora\n",
    "* Performing topic identification\n",
    "* Performing document comparison (retrieving semantically similar documents)\n",
    "* Analysing plain-text documents for semantic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "\n",
    "# import sys, os\n",
    "# import pandas as pd\n",
    "# from tika import parser # pip install tika\n",
    "# # from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # from nltk.stem import WordNetLemmatizer\n",
    "# # import re\n",
    "# # import glob\n",
    "# import numpy as np\n",
    "# # import matplotlib.pyplot as plt\n",
    "# from gensim import corpora\n",
    "# from gensim.models import LdaModel\n",
    "# # from operator import itemgetter\n",
    "# from nltk.corpus import stopwords\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# import nltk as nltk\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# directory = \"practice_pdfs\"\n",
    "# files = list(glob.glob(os.path.join(directory,'*.*')))\n",
    "# print(files)\n",
    "#https://stackoverflow.com/questions/34000914/how-to-create-a-list-from-filenames-in-a-user-specified-directory-in-python\n",
    "#https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "#https://stackoverflow.com/questions/33912773/python-read-txt-files-into-a-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file\n",
    "\n",
    "# document_list = []\n",
    "# for f in files:\n",
    "#     raw = parser.from_file(f)\n",
    "#     document_list.append(raw)\n",
    "\n",
    "# print(document_list)\n",
    "\n",
    "\n",
    "# raw = parser.from_file('CIR.0000000000000749.pdf')\n",
    "# # print(raw['content'])\n",
    "# # print(type(raw))\n",
    "# print(raw.keys())\n",
    "# metadata = raw[\"metadata\"]\n",
    "# content = raw[\"content\"]\n",
    "# # print(metadata)\n",
    "# # print(content)\n",
    "# print(type(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_df = pd.DataFrame(document_list)\n",
    "# text_df.head()\n",
    "# print(text_df[\"content\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "\n",
    "# def pre_process(text):\n",
    "    \n",
    "# #     text = \"<html> test, 'somthing, which I said' </html>\"\n",
    "#     #print(text)\n",
    "#     # lowercase\n",
    "#     text=text.lower()\n",
    "    \n",
    "# #     #remove tags\n",
    "# #     text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "#     #remove special characters and digits\n",
    "#     text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    " \n",
    "#     # remove special characters and space, but leave in periods and numbers\n",
    "#     #text=re.sub('[^A-Za-z0-9.]+|\\s',' ',text)\n",
    " \n",
    " \n",
    "# #     print(text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# text_df['preprocess'] = text_df['content'].apply(lambda x:pre_process(x))\n",
    "# # list(text_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_token_list=[]\n",
    "    \n",
    "# def tokenize_stem(documents):\n",
    "    \n",
    "#     # Create PorterStemmer\n",
    "#     #p_stemmer = PorterStemmer()\n",
    "#     #https://www.nltk.org/howto/stem.html\n",
    "    \n",
    "#     #Create SnowballStemmer\n",
    "#     sb_stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "\n",
    "#     #Open stop words text file and save to stop_set variable\n",
    "#     with open(\"stop_words.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "#         stopwords = f.readlines()\n",
    "#         stop_set = set(m.strip() for m in stopwords)\n",
    "#         f.close()\n",
    "\n",
    "# #     for i in range(0, len(documents)):\n",
    "# #         print(i)\n",
    "        \n",
    "#     for doc in documents.dropna():\n",
    "        \n",
    "#         ## Tokenize documents by splitting into words using NLTK's word_tokenize \n",
    "#         token_list = nltk.word_tokenize(doc)\n",
    "# #         print('Token_List: ', token_list)\n",
    "# #         print('-----------------------------------------------')\n",
    "\n",
    "#         ## Remove stop words from token_list\n",
    "#         token_nostop_list = [i for i in token_list if not i in stop_set]\n",
    "# #         print('Token_NoStop_List: ',token_nostop_list)\n",
    "# #         print('-----------------------------------------------')\n",
    "\n",
    "#         ## Use Porter Stemmer to stem tokens to create more like-words\n",
    "#         ##token_stem_list = [p_stemmer.stem(i) for i in token_nostop_list if len(i) > 3]\n",
    "            \n",
    "            \n",
    "#         ## Use Snowball Stemmer to stem tokens to create more like-words\n",
    "#         token_stem_list = [sb_stemmer.stem(i) for i in token_nostop_list if len(i) > 3]\n",
    "#         print('Token_Stem_List: ',token_stem_list)\n",
    "#         print('-----------------------------------------------')\n",
    "#         ##Append token_stem_list to the doc_token_list\n",
    "#         doc_token_list.append(token_stem_list)\n",
    "\n",
    "\n",
    "#     return doc_token_list\n",
    "\n",
    "# tokenize_stem(text_df['preprocess'])\n",
    "# print(type(doc_token_list))\n",
    "# # print(doc_token_list[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##If content is not yet a list, make it a list and build the id2word dictionary and the corpus (map the word to id)\n",
    "    ##texts = text_df['content'].apply(lambda x: x.split(' ')).tolist()\n",
    "    ##print(texts)\n",
    "\n",
    "##Build the id2word dictionary and the corpus\n",
    "##The dictionary associates each word in the corpus with a unique integer ID\n",
    "# dictionary = corpora.Dictionary(doc_token_list)\n",
    "#     #print('number of unique tokens: ', len(dictionary))\n",
    "\n",
    "# ## Filter out words that appear in less than 2 documents (appear only once),\n",
    "# dictionary.filter_extremes(no_below = 2)\n",
    "\n",
    "# ## Filter out words that appears in more than certain % of documents\n",
    "# ## no_above = 0.5 would remove words that appear in more than 50% of the documents\n",
    "# # dictionary.filter_extremes(no_above = 0.5)\n",
    "\n",
    "# # Remove gaps in id sequence after words that were removed\n",
    "# dictionary.compactify()\n",
    "# #print('number of unique tokens used 2 or more times: ', len(dictionary))\n",
    "\n",
    "# ##Use code below to print terms in dictionary with their IDs\n",
    "# pprint.pprint(dictionary.token2id)\n",
    "    \n",
    "# ##Map terms in corpus to words in dictionary with ID\n",
    "# corpus = [dictionary.doc2bow(text) for text in doc_token_list]\n",
    "\n",
    "# # Directory for storing all lda models\n",
    "# model_dir = 'lda_checkpoint'\n",
    "\n",
    "# if not os.path.isdir(model_dir):\n",
    "#     os.mkdir(model_dir)\n",
    "\n",
    "# # Load the model if we've already trained it before\n",
    "# n_topics = 3\n",
    "# path = os.path.join(model_dir, 'topic_model.lda')\n",
    "\n",
    "# if not os.path.isfile(path):\n",
    "#     # training LDA can take some time, we could set eval_every = None to not evaluate the model perplexity\n",
    "#     topic_model = LdaModel(\n",
    "#     corpus, id2word = dictionary, num_topics = 3, iterations = 200)\n",
    "#     topic_model.save(path)\n",
    "\n",
    "# topic_model = LdaModel.load(path)\n",
    "\n",
    "# # Each element of the list is a tuple containing the topic and word / probability list\n",
    "# topics = topic_model.show_topics(num_words = 10, formatted = False)\n",
    "\n",
    "# print(type(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py:55: DeprecationWarning: PILLOW_VERSION is deprecated and will be removed in a future release. Use __version__ instead.\n",
      "  from PIL import PILLOW_VERSION\n",
      "C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py:55: DeprecationWarning: PILLOW_VERSION is deprecated and will be removed in a future release. Use __version__ instead.\n",
      "  from PIL import PILLOW_VERSION\n",
      "C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "import pprint\n",
    "import sys, os\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk as nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "data = newsgroups_train.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from guy kuo subject si clock poll final call summary final call for si clock reports keywords si acceleration clock upgrade article i d shelley qvfo innc s organization university of washington lines nntp posting host carson u washington edu a fair number of brave souls who upgraded their si clock oscillator have shared their experiences for this poll please send a brief message detailing your experiences with the procedure top speed attained cpu rated speed add on cards and adapters heat sinks hour of usage per day floppy disk functionality with and m floppies are especially requested i will be summarizing in the next two days so please add to the network knowledge base if you have done the clock upgrade and haven t answered this poll thanks guy kuo \n"
     ]
    }
   ],
   "source": [
    "## Data Pre-processing to remove special characters, numbers, and emails\n",
    "\n",
    "data_pre_process = []\n",
    "\n",
    "def preprocess_data(data): \n",
    "    \n",
    "    for email in data:\n",
    "        \n",
    "        ##lowercase\n",
    "        text_lower=email.lower()\n",
    "        \n",
    "        ##Remove Emails\n",
    "        text_email=re.sub('\\\\S*@\\\\S*\\\\s?', '', text_lower) \n",
    "        \n",
    "        ##remove special characters and digits\n",
    "        text_special=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_email)\n",
    "        \n",
    "        ## remove special characters and space, but leave in periods and numbers\n",
    "        #text=re.sub('[^A-Za-z0-9.]+|\\s',' ',text)\n",
    "\n",
    "        ##remove tags\n",
    "        #text=re.sub(\"\",\"\",text)\n",
    "\n",
    "        ##Remove new line characters\n",
    "        #text=[re.sub('\\\\s+', ' ', text)\n",
    "\n",
    "        ##Remove distracting single quotes\n",
    "        #text=[re.sub(\"\\'\", \"\", text) \n",
    "  \n",
    "        data_pre_process.append(text_special)\n",
    "    \n",
    "    return data_pre_process\n",
    "\n",
    "preprocess_data(data)\n",
    "print(data_pre_process[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'si', 'clock', 'reports', 'keywords', 'si', 'acceleration', 'clock', 'upgrade', 'article', 'shelley', 'qvfo', 'innc', 'organization', 'university', 'of', 'washington', 'lines', 'nntp', 'posting', 'host', 'carson', 'washington', 'edu', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'si', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'please', 'send', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', 'and', 'floppies', 'are', 'especially', 'requested', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'haven', 'answered', 'this', 'poll', 'thanks', 'guy', 'kuo']\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the data using Gensim Utils Simple Preprocess\n",
    "\n",
    "data_words = []\n",
    "def tokenize(documents):\n",
    "    for doc in documents:\n",
    "        token_list = gensim.utils.simple_preprocess(str(doc), deacc=True)  # deacc=True removes punctuations\n",
    "        data_words.append(token_list)\n",
    "    return data_words\n",
    "\n",
    "\n",
    "tokenize(data_pre_process)\n",
    "# print(type(data_words))\n",
    "print(data_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'si', 'clock', 'reports', 'keywords', 'si', 'acceleration', 'clock', 'upgrade', 'article', 'shelley', 'qvfo', 'innc', 'organization', 'university', 'washington', 'lines', 'nntp', 'posting', 'host', 'carson', 'washington', 'edu', 'fair', 'number', 'brave', 'souls', 'who', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'poll', 'brief', 'message', 'detailing', 'experiences', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'cards', 'adapters', 'heat', 'sinks', 'hour', 'usage', 'day', 'floppy', 'disk', 'functionality', 'floppies', 'especially', 'requested', 'will', 'summarizing', 'days', 'add', 'network', 'knowledge', 'base', 'clock', 'upgrade', 'haven', 'answered', 'poll', 'thanks', 'guy', 'kuo']\n"
     ]
    }
   ],
   "source": [
    "## Remove Stopwords using a custom stopword list\n",
    "documents_nostop_list = []\n",
    "\n",
    "def remove_stopwords(documents):\n",
    "    \n",
    "    ##Open stop words text file and save to stop_set variable\n",
    "    with open(\"stop_words.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        f.close()\n",
    "\n",
    "    ##Stopword list comes from the Terrier pacakge with 733 words and another 86 custom terms: \n",
    "    ##https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt\n",
    "    ##https://github.com/kavgan/stop-words/blob/master/minimal-stop.txt\n",
    "    \n",
    "    ##Other stopword list options can be reviewed here:\n",
    "    ##https://medium.com/towards-artificial-intelligence/stop-the-stopwords-using-different-python-libraries-ffa6df941653\n",
    "\n",
    "\n",
    "    for doc in documents:\n",
    "\n",
    "        # Remove stop words from token_list\n",
    "        token_nostop_list = [i for i in doc if not i in stop_set]\n",
    "        \n",
    "        documents_nostop_list.append(token_nostop_list)\n",
    "        \n",
    "    return documents_nostop_list\n",
    "\n",
    "remove_stopwords(data_words)\n",
    "print(documents_nostop_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guy_kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'si', 'clock', 'reports', 'keywords', 'si', 'acceleration', 'clock', 'upgrade', 'article_shelley', 'qvfo', 'innc', 'organization', 'university', 'washington', 'lines', 'nntp_posting_host', 'carson_washington_edu', 'fair', 'number', 'brave', 'souls', 'who', 'upgraded', 'si', 'clock', 'oscillator', 'shared', 'experiences', 'poll', 'brief', 'message', 'detailing', 'experiences', 'procedure', 'top', 'speed', 'attained', 'cpu', 'rated', 'speed', 'add', 'cards', 'adapters', 'heat_sinks', 'hour', 'usage', 'day', 'floppy_disk', 'functionality', 'floppies', 'especially', 'requested', 'will', 'summarizing', 'days', 'add', 'network', 'knowledge', 'base', 'clock', 'upgrade', 'haven', 'answered', 'poll', 'thanks', 'guy_kuo']\n"
     ]
    }
   ],
   "source": [
    "## Create Bigram and Trigram Tokens from non-stop word data, and then compare to stopword\n",
    "\n",
    "bigram_token = []\n",
    "trigram_token = []\n",
    "\n",
    "def build_bigram_trigram_models(documents, documents_nostop):\n",
    "    \n",
    "    ##Building Bigram & Trigram Models\n",
    "    ##higher threshold fewer phrases.\n",
    "    bigram = gensim.models.Phrases(documents, min_count=5, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[documents], threshold=100)\n",
    "        \n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "        \n",
    "   \n",
    "    for doc in documents_nostop:\n",
    "        bigram_token.append(bigram_mod[doc])\n",
    "    \n",
    "    for doc in bigram_token:\n",
    "        trigram_token.append(trigram_mod[bigram_mod[doc]])\n",
    "        \n",
    "    return trigram_token\n",
    "\n",
    "\n",
    "build_bigram_trigram_models(data_words, documents_nostop_list)\n",
    "\n",
    "print(trigram_token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject', 'clock', 'poll', 'final', 'summary', 'final', 'call', 'si', 'clock', 'report', 'keyword', 'si', 'acceleration', 'clock', 'upgrade', 'nntp_posting_host', 'fair', 'number', 'brave', 'soul', 'upgrade', 'clock', 'oscillator', 'share', 'experience', 'brief', 'message', 'detail', 'experience', 'procedure', 'top', 'speed', 'attain', 'rated', 'speed', 'add', 'card', 'adapter', 'hour', 'usage', 'day', 'functionality', 'floppie', 'especially', 'request', 'will', 'summarize', 'day', 'add', 'network', 'knowledge', 'base', 'answer', 'poll', 'thank']\n"
     ]
    }
   ],
   "source": [
    "## Lemmetize the Data\n",
    "\n",
    "texts_out = []\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \n",
    "    nlp = spacy.load(r'C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.3.1')\n",
    "    #nlp = spacy.load('C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.3.1', disable=['parser', 'ner'])\n",
    "    \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    \n",
    "    return texts_out\n",
    "\n",
    "\n",
    "lemmatization(trigram_token, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(texts_out[1])\n",
    "\n",
    "#pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "#https://stackoverflow.com/questions/54334304/spacy-cant-find-model-en-core-web-sm-on-windows-10-and-python-3-5-3-anacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens:  39350\n",
      "Number of unique tokens used 2 or more times:  22502\n",
      "Word Count in each Vector: \n",
      "[('call', 1),\n",
      " ('day', 2),\n",
      " ('nntp_posting_host', 1),\n",
      " ('thank', 1),\n",
      " ('acceleration', 1),\n",
      " ('adapter', 1),\n",
      " ('add', 2),\n",
      " ('answer', 1),\n",
      " ('attain', 1),\n",
      " ('base', 1),\n",
      " ('brave', 1),\n",
      " ('brief', 1),\n",
      " ('card', 1),\n",
      " ('clock', 4),\n",
      " ('detail', 1),\n",
      " ('especially', 1),\n",
      " ('experience', 2),\n",
      " ('fair', 1),\n",
      " ('final', 2),\n",
      " ('floppie', 1),\n",
      " ('functionality', 1),\n",
      " ('hour', 1),\n",
      " ('keyword', 1),\n",
      " ('knowledge', 1),\n",
      " ('message', 1),\n",
      " ('network', 1),\n",
      " ('number', 1),\n",
      " ('oscillator', 1),\n",
      " ('poll', 2),\n",
      " ('procedure', 1),\n",
      " ('report', 1),\n",
      " ('request', 1),\n",
      " ('share', 1),\n",
      " ('si', 2),\n",
      " ('soul', 1),\n",
      " ('speed', 2),\n",
      " ('summarize', 1),\n",
      " ('summary', 1),\n",
      " ('top', 1),\n",
      " ('upgrade', 2),\n",
      " ('usage', 1),\n",
      " ('will', 1)]\n"
     ]
    }
   ],
   "source": [
    "##Run the gensim topic modeling and return the topics\n",
    "##Code from: https://notebook.community/ethen8181/machine-learning/clustering/topic_model/LDA\n",
    "\n",
    "def get_gensim_corpus_dictionary(data):\n",
    "    ##If content is not yet a list, make it a list and build the id2word dictionary and the corpus (map the word to id)\n",
    "    ##texts = text_df['content'].apply(lambda x: x.split(' ')).tolist()\n",
    "    ##print(texts)\n",
    "\n",
    "    ##Build the id2word dictionary and the corpus\n",
    "    ##The dictionary associates each word in the corpus with a unique integer ID\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    print('Number of unique tokens: ', len(dictionary))\n",
    "\n",
    "    ## Filter out words that appear in less than 2 documents (appear only once),\n",
    "    dictionary.filter_extremes(no_below = 2)\n",
    "\n",
    "    ## Filter out words that appears in more than certain % of documents\n",
    "    ## no_above = 0.5 would remove words that appear in more than 50% of the documents\n",
    "    # dictionary.filter_extremes(no_above = 0.5)\n",
    "\n",
    "    # Remove gaps in id sequence after words that were removed\n",
    "    dictionary.compactify()\n",
    "    print('Number of unique tokens used 2 or more times: ', len(dictionary))\n",
    "\n",
    "    ##Use code below to print terms in dictionary with their IDs\n",
    "    ##This will show you the number of the terms in the dictionary\n",
    "    #print(\"Dictionary Tokens with ID: \")\n",
    "    #pprint.pprint(dictionary.token2id)\n",
    "    \n",
    "    ##Map terms in corpus to words in dictionary with ID\n",
    "    ##This will show you the ID of the term in the dictionary, and the number of times the terms occurs in the corpus\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in data]\n",
    "    #print(\"Tokens in Corpus with Occurrence: \")\n",
    "    #pprint.pprint(corpus)\n",
    "    \n",
    "    ##Print word count by vector \n",
    "    id_words_count = [[(dictionary[id], count) for id, count in line] for line in bow_corpus]\n",
    "    print(\"Word Count in each Vector: \")\n",
    "    pprint.pprint(id_words_count[1])\n",
    "    \n",
    "     \n",
    "    return bow_corpus, dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bow_corpus, dictionary = get_gensim_corpus_dictionary(texts_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "## Run the Gensim Library LDA Model\n",
    "## See link below if you want to save and load a model\n",
    "## https://notebook.community/ethen8181/machine-learning/clustering/topic_model/LDA\n",
    "\n",
    "def run_gensim_LDA_model(corpus, dictionary):\n",
    "    ##Directory for storing all lda models\n",
    "    model_dir = 'lda_checkpoint'\n",
    "\n",
    "    ##If model_dir directionry is not in the folder, then make the directory\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    ##Load the model if we've already trained it before\n",
    "   \n",
    "    path = os.path.join(model_dir, 'gensim_tutorial_topic_model.lda')\n",
    "    if not os.path.isfile(path):\n",
    "        ##Training LDA can take some time, we could set eval_every = None to not evaluate the model perplexity\n",
    "        ##Other parameters for LdaModel, include: random_state=100, update_every=1,chunksize=100,passes=10,alpha='auto',per_word_topics=True\n",
    "        topic_model = LdaModel(corpus, id2word = dictionary, num_topics = 20, iterations = 200, per_word_topics=True)\n",
    "        topic_model.save(path)\n",
    " \n",
    "    topic_model = LdaModel.load(path)\n",
    "\n",
    "    # Each element of the list is a tuple containing the topic and word / probability list\n",
    "    topics = topic_model.show_topics(num_words = 10, formatted = False)\n",
    "\n",
    "    print(type(topics))\n",
    "    \n",
    "  \n",
    "    \n",
    "    return topic_model, topics\n",
    "\n",
    "topic_model, topics = run_gensim_LDA_model(bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topics to CSV\n",
    "\n",
    "def create_topic_CSV(topics):\n",
    "    \n",
    "    ##Create dataframe for topics\n",
    "    df_topics = pd.DataFrame(topics, columns = ['TopicNum', 'Terms'])\n",
    "    #df_topics.head()\n",
    "\n",
    "    ## Save dataframe to csv\n",
    "    with open(r\"gensim_tutorial_topic_modeling.csv\", 'w', encoding='utf-8') as file:\n",
    "        df_topics.to_csv(file)\n",
    "        file.close()\n",
    "    \n",
    "create_topic_CSV(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.306579606253914\n",
      "\n",
      "Coherence Score:  0.4882871346781205\n"
     ]
    }
   ],
   "source": [
    "## Test Model Perplexity and Coherence\n",
    "\n",
    "def model_perplexity_coherence(bow_corpus, dictionary, texts_out, topic_model):\n",
    "    \n",
    "    ##Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. \n",
    "    ##In my experience, topic coherence score, in particular, has been more helpful.\n",
    "    #https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#:~:text=Topic%20Modeling%20is%20a%20technique,in%20the%20Python's%20Gensim%20package.\n",
    "\n",
    "    \n",
    "    ##The LDA model (lda_model) we have created above can be used to compute the model’s perplexity, \n",
    "    ##i.e. how good the model is. The lower the score the better the model will be.\n",
    "    # Compute Perplexity\n",
    "    perplexity_lda = topic_model.log_perplexity(bow_corpus)\n",
    "    print('\\nPerplexity: ',  perplexity_lda)  # a measure of how good the model is. lower the better.\n",
    "    \n",
    "    ## Compute Coherence Score\n",
    "#     coherence_model_lda = CoherenceModel(model=topic_model, texts=corpus, dictionary=dictionary, coherence='c_v')\n",
    "#     coherence_lda = coherence_model_lda.get_coherence()\n",
    "#     print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "    ##The LDA model (lda_model) we have created above can be used to compute the model’s coherence score \n",
    "    ##i.e. the average /median of the pairwise word-similarity scores of the words in the topic. \n",
    "    \n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=topic_model, texts=texts_out, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    \n",
    "    return perplexity_lda, coherence_lda\n",
    "\n",
    "perplexity_lda, coherence_lda = model_perplexity_coherence(bow_corpus, dictionary, texts_out, topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
